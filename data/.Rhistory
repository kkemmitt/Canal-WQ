library(car) # if anybody has problems with this, try install dependencies manually, lme4 is the problematic one
library(ggplot2) # this will  need to be installed
library(pwr) # this will  need to be installed
nn1=6#sample size of sample1
nn2=12#sample size of sample 2
class(nn1) # nn1 is just a number
sample1=c( 8.736339,55.463842,62.358608,47.560199,47.525478,32.294160) #we create (concatenate) elements into a vector
length(sample1) # we could have specified nn1 as the length of the vector
sample2=c(18.08182,45.05992,33.45347,22.45844,33.16731,22.19066,17.68734,14.81729,35.17519,35.23651,45.98866,48.19830)
summary(sample1) #summary stats: different means!
summary(sample2)
yy=c(sample1,sample2) # stacking y values into a vector
yy
cats=c(rep('sample1',nn1),rep('sample2',nn2)) # stacking the categorical or classifying variables
cats
xx=factor(cats) # making cats a factor for a t-test
class(xx) # yes, we have created a factor
dataStack=data.frame(cbind(xx,yy))# c olumn stacked data #xx is factors and yy is data corresponding
class(dataStack) # data frames are "lists of vectors", can contain characters and numbers and are very useful/flexible for storing data tables
View(dataStack)
cats
xx
class(xx)
sample1=c(sample1,rep(NA,(max(nn1,nn2))-nn1)) # making two equal length vectors to avoid sparse matrix, adding NA's so that sample 1 and 2 have same length
sample1
sample2=c(sample2,rep(NA,(max(nn1,nn2))-nn2)) # making two equal length vectors to avoid sparse matrix
sample2
data=cbind(sample1,sample2) # making a matrix with groups by column
data # this is what we just created
boxplot(data) # visualize data
resids=(scale(data)[,1:2]) # normalized residuals
qqPlot(resids) # normality assessment
Ftest=max(var(sample1,na.rm = TRUE),var(sample2,na.rm = TRUE))/min(var(sample1,na.rm = TRUE),var(sample2,na.rm = TRUE))
sigFtest=2*pf(Ftest,max(nn1,nn2),min(nn1,nn2),lower.tail = FALSE) # pf = distribution function, need to specify F, multiplying by 2 to get two tailed test degrees of freedom 1 and 2 #want p>.05
sigFtest # are we good to proceed?
signal=(mean(sample1,na.rm = TRUE)-mean(sample2,na.rm = TRUE)) # numerator of the t statistic (mean difference)
sp2=sum((sample1-mean(sample1,na.rm = TRUE))^2,(sample2-mean(sample2,na.rm = TRUE))^2,na.rm = TRUE)/(nn1+nn2-2) # for the denominator we first need to compute the pooled variance
noise=sqrt((sp2/nn1)+(sp2/nn2)) # denominator of the t statistic
tStat=signal/noise
tStat
sigT=2*pt(tStat,(nn1+nn2-2),lower.tail=FALSE)
sigT # reject or accept H0?
attach(dataStack) # need to set up dataStack in WG
out1=t.test(yy~xx,alternative="two.sided",mu=0,paired=FALSE,var.equal=TRUE) # lots of options here, same result!
out1
?t.test
cohen.d=signal/sqrt(sp2) # Cohen's d is defined as the difference between two means divided by a standard deviation for the data
cohen.d
Power.U=pwr.t2n.test(n1=nn1,n2=nn2,d=cohen.d,power=NULL,sig.level=0.05)# two saample t test function for assessing power
calcPower=Power.U$power
calcPower
?rnorm
s1<-rnorm(100, mean=0, sd=2000)
s2<-rnorm(100, mean=100, sd=4000)
n=100 # number of points
a=100
b1=3 #slopes
b2=0.001
b3=0.1
MSE1=0.5
MSE2=3*MSE1
MSE3=3*MSE2
x1 <- rnorm(n,mean=30,sd=9)#using mean not equal zero here to center distribution away from zero
x2 <- x1^2#ditto
x3<- x1^3#this one has log normal
X<- cbind(x1, x2, x3)#matrix of xs
X
boxplot(X)
y <- a + b1*X[,1] + b2*X[,2] + b3*X[,3]+rnorm(n, 0, MSE2)
df <- data.frame(response=y, driver1=X[,1],driver2=X[,2],driver3=X[,3])
write.csv(df,file="mlRegData1new.csv")
data=read.csv("mlRegData1new.csv")
attach(data)
op <- par(mfrow = c(3, 1), mar=c(3,3,3,3))
plot(driver1,response,main="X1 vs Y")
plot(driver2,response,main="X2 vs Y")
plot(driver3,response,main="X3 vs Y")
par(op)
XX=cbind(driver1,driver2,driver3)
pairs(data)
pairs(XX)
outA<-lm(response~driver1+driver2+driver3)
op <- par(mfrow = c(2, 2), # 2 x 2 pictures on one plot
pty = "m")       # square plotting region,
# independent of device size
plot(outA)
par(op)
library(car)
qqPlot(scale(outA$residuals)[,1])#standard QQ for normality
outlierTest(outA) # Bonferonni p-value for most extreme obs
leveragePlots(outA) # leverage plots
cutoff <- 4/((n-length(outA$coefficients)-2))
plot(outA, which=4, cook.levels=cutoff)#plots cooks D, dont want these to be greater than 4/n
outCor<-cor(X)#correlation coeficients, look for R > 0.4 (conservative) or R>0.6 (acceptable)
outCor
tCor<-outCor/sqrt((1-outCor^2)/(n-2))#t-values for R
pCor<-1-pt(tCor,n-2)#significance testing for Rs, remember balance between sample size and significance
pCor
vif(outA) # variance inflation factors
vif(outA)>5 #(want these to be false, less than 5)
sqrt(vif(outA)) > 2 # want these to be false (less than ~2)
data=read.csv("mlRegData1new.csv")
attach(data)
y=response
x1=driver1
x2=driver2
x3=driver3
summary(outA)#driver2 isn't important
out1=lm(y~x1+x2+x3)#full model (5 coef)
out2=lm(y~x1+x2)#4 coef model
out3=lm(y~x1+x3)#4 coef model
out4=lm(y~x2+x3)#4 coef model
out5=lm(y~x1)#3 coef model
out6=lm(y~x2)#3 coef model
out7=lm(y~x3)#3 coef model
aics=c(AIC(out1),AIC(out2),AIC(out3),AIC(out4),AIC(out5),AIC(out6),AIC(out7))
aics
delAICs=aics-min(aics)
delAICs
likes=exp(-0.5*delAICs)
likes
ws=round(likes/sum(likes),3)
ws
matWS=cbind(ws,ws,ws,ws)
coeff1=summary(out1)$coefficients[,1]
coeff2=c(summary(out2)$coefficients[,1],NA)
coeff3=c(summary(out3)$coefficients[1:2,1],NA,summary(out3)$coefficients[3,1])
coeff4=c(summary(out4)$coefficients[1,1],NA,summary(out4)$coefficients[2:3,1])
coeff5=c(summary(out5)$coefficients[,1],NA,NA)
coeff6=c(summary(out6)$coefficients[1,1],NA,summary(out6)$coefficients[1,2],NA)
coeff7=c(summary(out7)$coefficients[1,1],NA,NA,summary(out7)$coefficients[1,2])
coeffs=rbind(coeff1,coeff2,coeff3,coeff4,coeff5,coeff6,coeff7)
matWS=cbind(ws,ws,ws,ws)
wcoeffs=colSums(ws*coeffs,na.rm = TRUE)
wcoeffs #include terms that are important
ws
step=step(lm(y~x1+x2+x3), direction="backward", trace=1)
coeffs=rbind(coeff1,coeff2,coeff3,coeff4,coeff5,coeff6,coeff7)
matWS=cbind(ws,ws,ws,ws)
binaryCoef=coeffs/coeffs
vWeights=coljSums(ws*binaryCoef,na.rm=TRUE)
vWeights=colSums(ws*binaryCoef,na.rm=TRUE)
vWeights
wcoeffs=colSums(ws*coeffs,na.rm = TRUE)
wcoeffs=colSums(ws*coeffs
)
ws*coeffs
wcoeffs
matWS=cbind(ws,ws,ws,ws)
aics=c(AIC(out1),AIC(out2),AIC(out3),AIC(out4),AIC(out5),AIC(out6),AIC(out7))
aics
delAICs=aics-min(aics)
delAICs
likes=exp(-0.5*delAICs)
likes
ws=round(likes/sum(likes),3)
ws
matWS=cbind(ws,ws,ws,ws)
coeffs=rbind(coeff1,coeff2,coeff3,coeff4,coeff5,coeff6,coeff7)
coeffs=rbind(coeff4,coeff6,coeff7)
matWS=cbind(ws,ws,ws,ws)
binaryCoef=coeffs/coeffs
vWeights=colSums(ws*binaryCoef,na.rm=TRUE)
vWeights
wcoeffs=colSums(ws*coeffs,na.rm = TRUE)
wcoeffs
a=150
b1=17.8
b2=-0.5
b3=0.08
x=1:1:35
N=length(x)
sigmaa=25
x
N=length(x)
noise=rnorm(N,mean=0,sd=sigmaa)
noise
yLin=a+b1/4*x
yLinC=yLin+noise
plot(yLin)
yQuad=a+b1*x+b2*x^2
yQuadC=yQuad+noise
yQuadC
yCube=1.1*(a*3.5+b1*x+b2*(6)*x^2+b3*x^3)
yCubeC=yCube+noise
op=par(mfrow=(c(3,1)),cex.axis=1.5,cex.lab=1.5)
plot(yLin,type="l",col="black",lwd=2,xlab="",ylim=c(100,500))
points(x,yLinC,pch=16,col="red")
plot(yQuad,col="black",type="l",lwd=2,xlab="",ylim=c(100,500))
plot(yCube,col="black",type="l",lwd=2,xlab="Driver",ylim=c(0,1000))
points(x,yCubeC,pch=16,col="red")
points(x,yQuadC,pch=16,col="red")
par(op)
a=150
b1=17.8
b2=-0.5
b3=0.08
x=1:1:35
N=length(x)
sigmaa=25
#sigmaa=200
noise=rnorm(N,mean=0,sd=sigmaa)
#linear
yLin=a+b1/4*x
yLinC=yLin+noise
plot(yLin)
#Quadratic
yQuad=a+b1*x+b2*x^2
yQuadC=yQuad+noise
#Cubic
yCube=1.1*(a*3.5+b1*x+b2*(6)*x^2+b3*x^3)
yCubeC=yCube+noise
op=par(mfrow=(c(3,1)),cex.axis=1.5,cex.lab=1.5)
plot(yLin,type="l",col="black",lwd=2,xlab="",ylim=c(100,500))
points(x,yLinC,pch=16,col="red")
plot(yQuad,col="black",type="l",lwd=2,xlab="",ylim=c(100,500))
points(x,yQuadC,pch=16,col="red")
plot(yCube,col="black",type="l",lwd=2,xlab="Driver",ylim=c(0,1000))
points(x,yCubeC,pch=16,col="red")
par(op)
o1=lm(yCubeC~ x)#linear
o2=lm(yCubeC~ x + I(x^2))#quadratic
o3=lm(yCubeC~ x + I(x^2) + I(x^3))#cubic
aicLin=AIC(o1)
aicQuad=AIC(o2)
aicCub=AIC(o3)
#Now lets see which of the three best fits the quad dataset
#note that this is one realization, so might end up being non-quadratic by chance
o4=lm(yQuadC~ x)#linear
o5=lm(yQuadC~ x + I(x^2))#quadratic
o6=lm(yQuadC~ x + I(x^2) + I(x^3))#cubic
aicLin2=AIC(o4)
aicQuad2=AIC(o5)
aicCub2=AIC(o6)
aics=rbind(aicLin2, aicQuad2, aicCub2)
delAICs=aics-min(aics)
likes=exp(-0.5*delAICs)
delAICs
wss=likes/sum(likes)
wss
varWss=rbind(sum(wss), sum(wss[2:3]), sum(wss[3]))
summary(o4)$coefficients
summary(o6)$coefficients
library(streamMetabolizer)
library(httr)
devtools::install_github("USGS-R/streamMetabolizer@develop")
library(httr)
library(jsonlite)
library(tidyr)
library(dplyr)
library(devtools)
suppressMessages(source_gist("2626629e50d191394285a61a0c678779"))
fitdata <- sp_metab(sitecode = "AZ_OC",
startdate = "2016-11-14",
enddate = "2016-12-09",
type = "mle",
token = NULL)
library(tidyverse)
# Import the data using the read_csv function that is part of the tidyverse
# package. In most database systems, lack of data (not zero, but specifically a
data<-read.csv("617_sampleNames.csv")
setwd("C:/Users/Katie Kemmitt/OneDrive/Research/Drinking-Water-Canal-Data/data")
library(streamMetabolizer)
prac<-read.csv("Sensor.Readings.Presssure.csv")
names(prac)[names(prac)=="DO"] <- "perc.sat"
names(prac)[names(prac)=="Temp"] <- "temp"
names(prac)[names(prac)=="press.avg.mbar"] <- "measured.atmP"
O2.saturation<-function(salinity, temp, measured.atmP, perc.sat) {
a=4.9e1
b=-1.335
c=2.759e-2
d=-3.235e-4
e=1.598e-6
p=5.516e-1
q=-1.759e-2
r=2.253e-4
s=-2.654e-7
t=5.362e-8
A=5.257e1
B=6.69e3
C=4.681
TK=temp+273
Chloride=(salinity-0.03)/1.805
atmPsealevel=1013
MolVol=22.414
MWO2=32
alpha=a+(b*temp)+(c*temp^2)+(d*temp^3)+(e*temp^4)-(Chloride*(p+(q*temp)+(r*temp^2)+(s*temp^3)+(t*temp^4)))
bunsen=alpha/1000
vapP=exp(A-(B/TK)-(C*log(TK)))
umoleO2.per.L<-(((measured.atmP-vapP)/atmPsealevel)*(perc.sat/100)*0.2095*bunsen*1e6*(1/MolVol))
mgO2.per.L<-umoleO2.per.L*(MWO2/1000)
pO2.torr<-((measured.atmP-vapP)*((perc.sat/100)*0.2095))*0.75
pO2.mbar<-pO2.torr/0.75
pO2.kPa<-pO2.mbar/10
output<-data.frame(salinity, temp, measured.atmP, perc.sat, umoleO2.per.L, mgO2.per.L, pO2.torr, pO2.mbar, pO2.kPa)
print(output)
}
SOCA <- prac[ which(prac$Sensor.Name=='Wql_030002_Probe1'),]
salinity=3 ##guestimate of freshwater salinity
perc.sat=100
o2.sat.100<-c()
for(i in 1:nrow(SOCA))
{
temp[i]<-SOCA[i,]$temp
measured.atmP[i]<-SOCA[i,]$measured.atmP
o2.sat.100<-O2.saturation(salinity, temp, measured.atmP, perc.sat)
}
attach(SOCA)
for(i in 1:nrow(SOCA))
{
temp[i]<-SOCA[i,]$temp
measured.atmP[i]<-SOCA[i,]$measured.atmP
o2.sat.100<-O2.saturation(salinity, temp, measured.atmP, 100)
}
str(SOCA)
for(i in 1:nrow(SOCA))
for(i in 1:nrow(SOCA))
{
temp[i]<-SOCA[i,]$temp
measured.atmP[i]<-SOCA[i,]$measured.atmP
o2.sat.100<-O2.saturation(salinity, temp, measured.atmP, perc.sat)
}
View(o2.sat.100)
SOCA$o2.sat.100<-o2.sat.100$mgO2.per.L #insert calculations into dataframe
prac$o2.mg.L<- (prac$perc.sat * prac$o2.sat.100)/100
prac$o2.mg.L<- (prac$perc.sat * prac$o2.sat.100)/100
SOCA$o2.mg.L<- (SOCA$perc.sat * SOCA$o2.sat.100)/100
prac$depth=4
prac$light=900
SOCA$depth=4
SOCA$light=900
names(SOCA)[names(SOCA)=="o2.sat.100"] <- "DO.sat"
names(SOCA)[names(SOCA)=="o2.mg.L"] <- "DO.obs"
names(SOCA)[names(SOCA)=="posix.time.solar"]<-"solar.time"
names(SOCA)[names(SOCA)=="temp"]<-"temp.water"
O2dataSOCA<-SOCA[, c("solar.time", "DO.obs", "DO.sat", "depth", "temp.water", "light")]
SOCA$Reading.Date<-as.POSIXct(SOCA$Reading.Date, format = "%m/%d/%Y %H:%M", tz='Etc/GMT+7')
lubridate::tz(SOCA$Reading.Date)#checking that time is in right tz
(SOCA$solar.time <- streamMetabolizer::calc_solar_time(SOCA$Reading.Date, longitude=-106.3))
O2dataSOCA<-SOCA[, c("solar.time", "DO.obs", "DO.sat", "depth", "temp.water", "light")]
mm_name(type='mle')
mle_name <- mm_name(type='mle')
mle_specs <- specs(mle_name)
mle_fit <- metab(mle_specs, data=O2dataSOCA)
met_preds <- predict_metab(mle_fit)# plot of GPP values from model
plot(met_preds$GPP ~ met_preds$date)
met_preds
mle_fit
mle_fit <- metab(mle_specs, data=O2dataSOCA)
mle_fit <- metab(mle_specs, data=O2dataSOCA)
O2dataSOCA[ order(O2dataSOCA$solar.time , decreasing = FALSE ),]
mle_fit <- metab(mle_specs, data=O2dataSOCA)
View(O2dataSOCA)
mle_specs
mle_fit <- metab(mle_specs, data=O2dataSOCA)
mm_name(type='mle')
mle_name <- mm_name(type='mle')
mle_name
mle_specs <- specs(mle_name)
mle_specs
mle_fit <- metab(mle_specs, data=O2dataSOCA)
mle_fit
data<-read.csv("SRP_data_1site.csv") #read data
data$Date.Time<-as.POSIXct(data$Date.Time, format = "%m/%d/%Y %H:%M", tz='Etc/GMT+7')
lubridate::tz(data$Date.Time)#checking that time is in right tz
(data$posix.time.solar <- streamMetabolizer::calc_solar_time(data$Date.Time, longitude=-106.3))
names(data)[names(data)=="DO_percent"] <- "perc.sat"
names(data)[names(data)=="Temp_C"] <- "temp"
names(data)[names(data)=="pressure_mbar"] <- "measured.atmP"
O2.saturation<-function(salinity, temp, measured.atmP, perc.sat) {
a=4.9e1
b=-1.335
c=2.759e-2
d=-3.235e-4
e=1.598e-6
p=5.516e-1
q=-1.759e-2
r=2.253e-4
s=-2.654e-7
t=5.362e-8
A=5.257e1
B=6.69e3
C=4.681
TK=temp+273
Chloride=(salinity-0.03)/1.805
atmPsealevel=1013
MolVol=22.414
MWO2=32
alpha=a+(b*temp)+(c*temp^2)+(d*temp^3)+(e*temp^4)-(Chloride*(p+(q*temp)+(r*temp^2)+(s*temp^3)+(t*temp^4)))
bunsen=alpha/1000
vapP=exp(A-(B/TK)-(C*log(TK)))
umoleO2.per.L<-(((measured.atmP-vapP)/atmPsealevel)*(perc.sat/100)*0.2095*bunsen*1e6*(1/MolVol))
mgO2.per.L<-umoleO2.per.L*(MWO2/1000)
pO2.torr<-((measured.atmP-vapP)*((perc.sat/100)*0.2095))*0.75
pO2.mbar<-pO2.torr/0.75
pO2.kPa<-pO2.mbar/10
output<-data.frame(salinity, temp, measured.atmP, perc.sat, umoleO2.per.L, mgO2.per.L, pO2.torr, pO2.mbar, pO2.kPa)
print(output)
}
salinity=3 ##guestimate of freshwater salinity
perc.sat=100
o2.sat.100<-c()
for(i in 1:nrow(data))
{
temp[i]<-data[i,]$temp
measured.atmP[i]<-data[i,]$measured.atmP
o2.sat.100<-O2.saturation(salinity, temp, measured.atmP, perc.sat)
}
View(o2.sat.100)
data$o2.sat.100<-o2.sat.100$mgO2.per.L #insert calculations into dataframe
data$o2.mg.L<- (data$perc.sat * data$o2.sat.100)/100
attach(data)
data$Date.Time<-as.POSIXct(data$Date.Time, format = "%m/%d/%Y %H:%M", tz='Etc/GMT+7')
data<-read.csv("SRP_data_1site.csv") #read data
data$Date.Time<-as.POSIXct(data$Date.Time, format = "%m/%d/%Y %H:%M", tz='Etc/GMT+7')
lubridate::tz(data$Date.Time)#checking that time is in right tz
(data$posix.time.solar <- streamMetabolizer::calc_solar_time(data$Date.Time, longitude=-106.3))
names(data)[names(data)=="DO_percent"] <- "perc.sat"
names(data)[names(data)=="Temp_C"] <- "temp"
names(data)[names(data)=="pressure_mbar"] <- "measured.atmP"
O2.saturation<-function(salinity, temp, measured.atmP, perc.sat) {
a=4.9e1
b=-1.335
c=2.759e-2
d=-3.235e-4
e=1.598e-6
p=5.516e-1
q=-1.759e-2
r=2.253e-4
s=-2.654e-7
t=5.362e-8
A=5.257e1
B=6.69e3
C=4.681
TK=temp+273
Chloride=(salinity-0.03)/1.805
atmPsealevel=1013
MolVol=22.414
MWO2=32
alpha=a+(b*temp)+(c*temp^2)+(d*temp^3)+(e*temp^4)-(Chloride*(p+(q*temp)+(r*temp^2)+(s*temp^3)+(t*temp^4)))
bunsen=alpha/1000
vapP=exp(A-(B/TK)-(C*log(TK)))
umoleO2.per.L<-(((measured.atmP-vapP)/atmPsealevel)*(perc.sat/100)*0.2095*bunsen*1e6*(1/MolVol))
mgO2.per.L<-umoleO2.per.L*(MWO2/1000)
pO2.torr<-((measured.atmP-vapP)*((perc.sat/100)*0.2095))*0.75
pO2.mbar<-pO2.torr/0.75
pO2.kPa<-pO2.mbar/10
output<-data.frame(salinity, temp, measured.atmP, perc.sat, umoleO2.per.L, mgO2.per.L, pO2.torr, pO2.mbar, pO2.kPa)
print(output)
}
attach(data)
